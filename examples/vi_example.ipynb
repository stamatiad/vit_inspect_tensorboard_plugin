{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4KJzl_XdLfl"
   },
   "source": [
    "\n",
    "# Example usage of VI with a PyTorch model.\n",
    "This is an example of VI usage on the DINO ViT model:\n",
    "`https://github.com/facebookresearch/dino`\n",
    ">This notebook comes as part of the `https://github.com/stamatiad/vit_inspect_tensorboard_plugin` repo and it assumes that you\n",
    " > run it in a linux/unix environment from within the `vit_inspect` folder.\n",
    "This notebook supports both running on the Colab cloud and locally in ipython. \n",
    "* In case you run locally, it takes care to clone any repositories, in the parent folder of `vit_inspect` repo. \n",
    "* In case it runs on Colab cloud, it should clone both `dino` and the `vit_inspect` inside the runtimes storage.\n",
    "\n",
    "To avoid restarting the Colab cloud runtime, and messing with cell order\n",
    "execution, we based our code on the preinstalled package versions of Colab.\n",
    "So no need to install any requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fH8aSz9X_1Ri"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Nj8gHq_-eMUT"
   },
   "outputs": [],
   "source": [
    "# Determine if you run in Colab cloud.\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    on_colab = True\n",
    "else:\n",
    "  on_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KHEyXX3AJIX",
    "outputId": "83a1c83a-ac2e-47b8-ca39-ad9df6e9512e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stama\\Documents\\GitHub\n"
     ]
    }
   ],
   "source": [
    "# Get out from the vit_inspect folder, into the parent directory of your git repositories:\n",
    "if not on_colab:\n",
    "  %cd ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_imDskhXd1sc"
   },
   "source": [
    "Now clone and install the VI plugin, if not done already. It is important to use setuptools for the TB to register the plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMWGUIkt1GBz",
    "outputId": "5ef3ac30-50f3-4987-f56d-ab0a83e6510e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stama\\Documents\\GitHub\\vit_inspect_tensorboard_plugin\n",
      "Already up to date.\n",
      "Processing c:\\users\\stama\\documents\\github\\vit_inspect_tensorboard_plugin\n",
      "Building wheels for collected packages: vit-inspect\n",
      "  Building wheel for vit-inspect (setup.py): started\n",
      "  Building wheel for vit-inspect (setup.py): finished with status 'done'\n",
      "  Created wheel for vit-inspect: filename=vit_inspect-0.1.0-py3-none-any.whl size=19195 sha256=646067297fdf66f5e5b96e89892a6fefc71c83c6071ba4e501e0c684a5d4356a\n",
      "  Stored in directory: C:\\Users\\stama\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-xtvypzwq\\wheels\\57\\1b\\42\\af0f5c7f58855909e5c471759a70a9aa418413499741d9dcb3\n",
      "Successfully built vit-inspect\n",
      "Installing collected packages: vit-inspect\n",
      "  Attempting uninstall: vit-inspect\n",
      "    Found existing installation: vit-inspect 0.1.0\n",
      "    Uninstalling vit-inspect-0.1.0:\n",
      "      Successfully uninstalled vit-inspect-0.1.0\n",
      "Successfully installed vit-inspect-0.1.0\n",
      "C:\\Users\\stama\\Documents\\GitHub\n"
     ]
    }
   ],
   "source": [
    "if not Path(os.getcwd()).joinpath('vit_inspect_tensorboard_plugin').exists():\n",
    "  !git clone https://stamatiad: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@github.com/stamatiad/vit_inspect_tensorboard_plugin.git\n",
    "%cd vit_inspect_tensorboard_plugin\n",
    "# Make sure repo is up to date:\n",
    "!git pull\n",
    "# Install using setup tools:\n",
    "!pip install --use-feature=in-tree-build .\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pO2-LoPdLf2"
   },
   "source": [
    "Now clone the repo of the PyTorch example model and checkout our customized branch to see\n",
    "the changes required to run it along with the VI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfCQhdRJdLf5",
    "outputId": "55f162b9-203d-466d-f132-4729164eda45",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stama\\Documents\\GitHub\\dino\n",
      "Already up to date.\n",
      "D\trequirements_colab.txt\n",
      "D\tvi_example.ipynb\n",
      "M\tvi_example.py\n",
      "D\tvi_logs/events.out.tfevents.1672074953.stamatiad-laptop.9563.0.v2\n",
      "D\tvi_logs/events.out.tfevents.1674386022.stamatiad-laptop.5141.0.v2\n",
      "M\tvision_transformer.py\n",
      "Your branch is up to date with 'origin/stamatiad'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Already on 'stamatiad'\n"
     ]
    }
   ],
   "source": [
    "if not Path(os.getcwd()).joinpath('dino').exists():\n",
    "  !git clone https://github.com/stamatiad/dino.git\n",
    "%cd dino\n",
    "# Make sure repo is up to date:\n",
    "!git pull\n",
    "# Checkout our custom branch, that integrates VI:\n",
    "!git checkout stamatiad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QMTg2nadLgn"
   },
   "source": [
    "As you can see, we have created a wrapper function (save_attn_weights) that\n",
    "wraps the forward method of Attention. Now each time the forward method is\n",
    "called and we have VI recording enabled (with vi.enable_vi() context manager)\n",
    ", we will save the TB summaries in the directory ./vi_logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TbwYCpyrdLgt",
    "outputId": "62cbda36-2eac-4244-ae4e-338be6774344",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff --git a/vision_transformer.py b/vision_transformer.py\n",
      "index f69a7ad..102e06d 100644\n",
      "--- a/vision_transformer.py\n",
      "+++ b/vision_transformer.py\n",
      "@@ -23,6 +23,70 @@ import torch.nn as nn\n",
      " \n",
      " from utils import trunc_normal_\n",
      " \n",
      "+# VI imports:\n",
      "+from functools import wraps\n",
      "+import numpy as np\n",
      "+import json\n",
      "+import tensorflow as tf\n",
      "+from vit_inspect import vit_inspector as vi\n",
      "+from vit_inspect.summary_v2 import vi_summary\n",
      "+\n",
      "+def save_attn_weights():\n",
      "+    # Zero indexed layer counter:\n",
      "+    layer_counter = 0\n",
      "+    def decorator(func):\n",
      "+        @wraps(func)\n",
      "+        def wrapper(*args, **kw):\n",
      "+            # Get access to the variable:\n",
      "+            nonlocal layer_counter\n",
      "+            try:\n",
      "+                # Evaluate the attention function first:\n",
      "+                x, w = func(*args, **kw)\n",
      "+                # If we are recording with VI:\n",
      "+                if vi._summary_is_active:\n",
      "+                    # Number of tokens:\n",
      "+                    nt = vi.params[\"num_tokens\"]\n",
      "+                    # Patch size\n",
      "+                    ps = int(np.sqrt(nt))\n",
      "+                    # Attention heads:\n",
      "+                    ah = vi.params[\"num_heads\"]\n",
      "+                    # Remove class dim and partition into image patches.\n",
      "+                    tmp = w[:, :, :nt, :nt].reshape(-1, ah, nt, ps, ps)\n",
      "+                    # Preprocess to image with cmap, in order to save with\n",
      "+                    # images TF writer.\n",
      "+                    attn_maps = vi.maps_to_imgs(tmp)\n",
      "+                    batch_id = vi.params[\"batch_id\"]\n",
      "+                    num_layers = vi.params[\"num_layers\"]\n",
      "+                    #Get active layer to add to the summary tag:\n",
      "+                    active_layer = np.mod(layer_counter, num_layers)\n",
      "+\n",
      "+                    # Convert to TF tensor, to save with summary writer:\n",
      "+                    flat_arr_w = tf.convert_to_tensor(\n",
      "+                        np.reshape(attn_maps, [-1, *attn_maps.shape[-3:]])\n",
      "+                    )\n",
      "+\n",
      "+                    with vi.writer.as_default():\n",
      "+                        vi_summary(\n",
      "+                            f\"b{batch_id}l{active_layer}\",\n",
      "+                            flat_arr_w,\n",
      "+                            step=vi.params[\"step\"],\n",
      "+                            # Provide all the relevant model params to the TB\n",
      "+                            # plugin:\n",
      "+                            description=json.dumps(vi.params)\n",
      "+                        )\n",
      "+                        vi.writer.flush()\n",
      "+\n",
      "+                    # Increment the layer counter, keeping track for the\n",
      "+                    # active layer.\n",
      "+                    layer_counter += 1\n",
      "+            except Exception as e:\n",
      "+                print(f\"Something in JAX broke: {e}\")\n",
      "+                raise e\n",
      "+            # Return the results of the attention function that we called\n",
      "+            # earlier:\n",
      "+            return x, w #=func(*args, **kw)\n",
      "+        return wrapper\n",
      "+    return decorator\n",
      " \n",
      " def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
      "     if drop_prob == 0. or not training:\n",
      "@@ -77,6 +141,7 @@ class Attention(nn.Module):\n",
      "         self.proj = nn.Linear(dim, dim)\n",
      "         self.proj_drop = nn.Dropout(proj_drop)\n",
      " \n",
      "+    @save_attn_weights()\n",
      "     def forward(self, x):\n",
      "         B, N, C = x.shape\n",
      "         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "!git diff main..stamatiad -- vision_transformer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZyraqundLg_"
   },
   "source": [
    "Now lets run our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "mLneb6WcdLhA"
   },
   "outputs": [],
   "source": [
    "# Work on the original DINO with PyTorch:\n",
    "\n",
    "# VI imports:\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from vit_inspect import vit_inspector as vi\n",
    "from vit_inspect.summary_v2 import vi_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gL12tmCZnJcs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as pth_transforms\n",
    "from vision_transformer import VisionTransformer\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c7C-oCYdLhE"
   },
   "source": [
    "Load the pre-trained model and transfer its parameters to a new instance of\n",
    "our modified model, that VI listens to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8KosQFNdLhG",
    "outputId": "e93f7079-7b0a-4f4b-fb69-aaf1d6124898",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\stama/.cache\\torch\\hub\\facebookresearch_dino_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained model:\n",
    "model_cached = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "# Create a version of the model that holds our attention VI modifications:\n",
    "# Match model params, before load:\n",
    "num_features = model_cached.embed_dim\n",
    "model = VisionTransformer(embed_dim=num_features)\n",
    "model.load_state_dict(model_cached.state_dict(), strict=False)\n",
    "\n",
    "# Enable evaluation mode:\n",
    "device = torch.device(\"cpu\")\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5WAtl-bdLhJ"
   },
   "source": [
    "Initialize the VI and inform it about our model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "6mxKiFZBdLhL"
   },
   "outputs": [],
   "source": [
    "# Get some model params, required for VI:\n",
    "vi.params[\"num_layers\"] = len(model.blocks)\n",
    "vi.params[\"num_heads\"] = model.blocks[0].attn.num_heads\n",
    "# The number of tokens when the attention dot product happens.\n",
    "# Here tokens are the patches. Any other feature (e.g. class) is removed.\n",
    "patch_size = model.patch_embed.patch_size\n",
    "crop_size = 480\n",
    "img_size_in_patches = crop_size // patch_size\n",
    "vi.params[\"len_in_patches\"] = img_size_in_patches\n",
    "# Total patches in the image:\n",
    "vi.params[\"num_tokens\"] = img_size_in_patches ** 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPAprINAdLhN"
   },
   "source": [
    "Load a sample image to calculate attention maps uppon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "j3Hsb-O_dLhO"
   },
   "outputs": [],
   "source": [
    "# Load sample images:\n",
    "response = requests.get(\"https://dl.fbaipublicfiles.com/dino/img.png\")\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img = img.convert('RGB')\n",
    "\n",
    "# Perform the original transformations that the authors did.\n",
    "transform = pth_transforms.Compose([\n",
    "    pth_transforms.Resize(img.size),\n",
    "    pth_transforms.ToTensor(),\n",
    "    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "img = transform(img)\n",
    "# make the image divisible by the patch size\n",
    "w, h = img.shape[1] - img.shape[1] % patch_size, \\\n",
    "       img.shape[2] - img.shape[2] % patch_size\n",
    "img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "w_featmap = img.shape[-2] // patch_size\n",
    "h_featmap = img.shape[-1] // patch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHWDEybLdLhP"
   },
   "source": [
    "Save a copy of the input image for the VI to display it as preview, making it\n",
    " easier to visualize the attention maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "lHdBYcvOdLhz"
   },
   "outputs": [],
   "source": [
    "# Save the input image into the summary:\n",
    "flat_arr_rgb = tf.convert_to_tensor(\n",
    "    # Make sure image's channels is the last dim:\n",
    "    np.moveaxis(np.asarray(img), 1, -1)\n",
    ")\n",
    "with vi.writer.as_default():\n",
    "    step = 0\n",
    "    batch_id = 0\n",
    "    vi.params[\"step\"] = 0\n",
    "    vi.params[\"batch_id\"] = batch_id\n",
    "    vi_summary(\n",
    "        f\"b{batch_id}\",\n",
    "        flat_arr_rgb,\n",
    "        step=step,\n",
    "        description=json.dumps(vi.params)\n",
    "    )\n",
    "    vi.writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rkAwA3LdLh1"
   },
   "source": [
    "Finally, perform inference with VI enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "JsWc0V9adLh2"
   },
   "outputs": [],
   "source": [
    "# Use the VI context manager to get attention maps of each layer and head:\n",
    "with vi.enable_vi():\n",
    "    attentions = model.get_last_selfattention(img.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "id": "03XpOC6ZdLh3",
    "outputId": "b3ebca87-3505-4b3f-c41e-88b6329ca5c1",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ykc8BiBcdLh4",
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir vi_logs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
