{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4KJzl_XdLfl"
   },
   "source": [
    "\n",
    "# Example usage of VI with a PyTorch model.\n",
    "This is an example of VI usage on the DINO ViT model:\n",
    "`https://github.com/facebookresearch/dino`\n",
    ">This notebook comes as part of the `https://github.com/stamatiad/vit_inspect_tensorboard_plugin` repo and it assumes that you\n",
    " > run it in a linux/unix environment from within the `vit_inspect` folder.\n",
    "This notebook supports both running on the Colab cloud and locally in ipython. \n",
    "* In case you run locally, it takes care to clone any repositories, in the parent folder of `vit_inspect` repo. \n",
    "* In case it runs on Colab cloud, it should clone both `dino` and the `vit_inspect` inside the runtimes storage.\n",
    "\n",
    "To avoid restarting the Colab cloud runtime, and messing with cell order\n",
    "execution, we based our code on the preinstalled package versions of Colab.\n",
    "So no need to install any requirements."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os, sys\n",
    "from pathlib import Path"
   ],
   "metadata": {
    "id": "fH8aSz9X_1Ri"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Determine if you run in Colab cloud.\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    on_colab = True\n",
    "else:\n",
    "  on_colab = False"
   ],
   "metadata": {
    "id": "Nj8gHq_-eMUT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Get out from the vit_inspect folder, into the parent directory of your git repositories:\n",
    "if not on_colab:\n",
    "  %cd ../"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KHEyXX3AJIX",
    "outputId": "83a1c83a-ac2e-47b8-ca39-ad9df6e9512e",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now clone and install the VI plugin, if not done already. It is important to use setuptools for the TB to register the plugin."
   ],
   "metadata": {
    "id": "_imDskhXd1sc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if not Path(os.getcwd()).joinpath('vit_inspect_tensorboard_plugin').exists():\n",
    "  !git clone https://stamatiad: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@github.com/stamatiad/vit_inspect_tensorboard_plugin.git\n",
    "%cd vit_inspect_tensorboard_plugin\n",
    "# Make sure repo is up to date:\n",
    "!git pull\n",
    "# Install using setup tools:\n",
    "!pip install .\n",
    "%cd ../"
   ],
   "metadata": {
    "id": "WMWGUIkt1GBz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5ef3ac30-50f3-4987-f56d-ab0a83e6510e",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pO2-LoPdLf2"
   },
   "source": [
    "Now clone the repo of the PyTorch example model and checkout our customized branch to see\n",
    "the changes required to run it along with the VI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfCQhdRJdLf5",
    "outputId": "55f162b9-203d-466d-f132-4729164eda45",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if not Path(os.getcwd()).joinpath('dino').exists():\n",
    "  !git clone https://github.com/stamatiad/dino.git\n",
    "%cd dino\n",
    "# Make sure repo is up to date:\n",
    "!git pull\n",
    "# Checkout our custom branch, that integrates VI:\n",
    "!git checkout stamatiad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QMTg2nadLgn"
   },
   "source": [
    "As you can see, we have created a wrapper function (save_attn_weights) that\n",
    "wraps the forward method of Attention. Now each time the forward method is\n",
    "called and we have VI recording enabled (with vi.enable_vi() context manager)\n",
    ", we will save the TB summaries in the directory ./vi_logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TbwYCpyrdLgt",
    "outputId": "62cbda36-2eac-4244-ae4e-338be6774344",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!git diff main..stamatiad -- vision_transformer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZyraqundLg_"
   },
   "source": [
    "Now lets run our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLneb6WcdLhA"
   },
   "outputs": [],
   "source": [
    "# Work on the original DINO with PyTorch:\n",
    "\n",
    "# VI imports:\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from vit_inspect import vit_inspector as vi\n",
    "from vit_inspect.summary_v2 import vi_summary"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as pth_transforms\n",
    "from vision_transformer import VisionTransformer\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from pathlib import Path"
   ],
   "metadata": {
    "id": "gL12tmCZnJcs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c7C-oCYdLhE"
   },
   "source": [
    "Load the pre-trained model and transfer its parameters to a new instance of\n",
    "our modified model, that VI listens to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8KosQFNdLhG",
    "outputId": "e93f7079-7b0a-4f4b-fb69-aaf1d6124898",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained model:\n",
    "model_cached = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "# Create a version of the model that holds our attention VI modifications:\n",
    "# Match model params, before load:\n",
    "num_features = model_cached.embed_dim\n",
    "model = VisionTransformer(embed_dim=num_features)\n",
    "model.load_state_dict(model_cached.state_dict(), strict=False)\n",
    "\n",
    "# Enable evaluation mode:\n",
    "device = torch.device(\"cpu\")\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5WAtl-bdLhJ"
   },
   "source": [
    "Initialize the VI and inform it about our model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mxKiFZBdLhL"
   },
   "outputs": [],
   "source": [
    "# Get some model params, required for VI:\n",
    "vi.params[\"num_layers\"] = len(model.blocks)\n",
    "vi.params[\"num_heads\"] = model.blocks[0].attn.num_heads\n",
    "# The number of tokens when the attention dot product happens.\n",
    "# Here tokens are the patches. Any other feature (e.g. class) is removed.\n",
    "patch_size = model.patch_embed.patch_size\n",
    "crop_size = 480\n",
    "img_size_in_patches = crop_size // patch_size\n",
    "vi.params[\"len_in_patches\"] = img_size_in_patches\n",
    "# Total patches in the image:\n",
    "vi.params[\"num_tokens\"] = img_size_in_patches ** 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPAprINAdLhN"
   },
   "source": [
    "Load a sample image to calculate attention maps uppon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3Hsb-O_dLhO"
   },
   "outputs": [],
   "source": [
    "# Load sample images:\n",
    "response = requests.get(\"https://dl.fbaipublicfiles.com/dino/img.png\")\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img = img.convert('RGB')\n",
    "\n",
    "# Perform the original transformations that the authors did.\n",
    "transform = pth_transforms.Compose([\n",
    "    pth_transforms.Resize(img.size),\n",
    "    pth_transforms.ToTensor(),\n",
    "    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "img = transform(img)\n",
    "# make the image divisible by the patch size\n",
    "w, h = img.shape[1] - img.shape[1] % patch_size, \\\n",
    "       img.shape[2] - img.shape[2] % patch_size\n",
    "img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "w_featmap = img.shape[-2] // patch_size\n",
    "h_featmap = img.shape[-1] // patch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHWDEybLdLhP"
   },
   "source": [
    "Save a copy of the input image for the VI to display it as preview, making it\n",
    " easier to visualize the attention maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHdBYcvOdLhz"
   },
   "outputs": [],
   "source": [
    "# Save the input image into the summary:\n",
    "flat_arr_rgb = tf.convert_to_tensor(\n",
    "    # Make sure image's channels is the last dim:\n",
    "    np.moveaxis(np.asarray(img), 1, -1)\n",
    ")\n",
    "with vi.writer.as_default():\n",
    "    step = 0\n",
    "    batch_id = 0\n",
    "    vi.params[\"step\"] = 0\n",
    "    vi.params[\"batch_id\"] = batch_id\n",
    "    vi_summary(\n",
    "        f\"b{batch_id}\",\n",
    "        flat_arr_rgb,\n",
    "        step=step,\n",
    "        description=json.dumps(vi.params)\n",
    "    )\n",
    "    vi.writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rkAwA3LdLh1"
   },
   "source": [
    "Finally, perform inference with VI enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsWc0V9adLh2"
   },
   "outputs": [],
   "source": [
    "# Use the VI context manager to get attention maps of each layer and head:\n",
    "with vi.enable_vi():\n",
    "    attentions = model.get_last_selfattention(img.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03XpOC6ZdLh3",
    "outputId": "b3ebca87-3505-4b3f-c41e-88b6329ca5c1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir vi_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ykc8BiBcdLh4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
