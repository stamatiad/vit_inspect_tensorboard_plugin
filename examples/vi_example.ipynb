{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4KJzl_XdLfl"
   },
   "source": [
    "\n",
    "# Example usage of VI with a PyTorch model.\n",
    "This is an example of VI usage on the DINO ViT model:\n",
    "`https://github.com/facebookresearch/dino`\n",
    ">This notebook comes as part of the `https://github.com/stamatiad/vit_inspect_tensorboard_plugin` repo and it assumes that you\n",
    " > run it in a linux/unix environment from within the `vit_inspect` folder.\n",
    "This notebook supports both running on the Colab cloud and locally in ipython. \n",
    "* In case you run locally, it takes care to clone any repositories, in the parent folder of `vit_inspect` repo. \n",
    "* In case it runs on Colab cloud, it should clone both `dino` and the `vit_inspect` inside the runtimes storage.\n",
    "\n",
    "To avoid restarting the Colab cloud runtime, and messing with cell order\n",
    "execution, we based our code on the preinstalled package versions of Colab.\n",
    "So no need to install any requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "fH8aSz9X_1Ri"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Nj8gHq_-eMUT"
   },
   "outputs": [],
   "source": [
    "# Determine if you run in Colab cloud.\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    on_colab = True\n",
    "else:\n",
    "  on_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/stamatiad/Documents/GitHub/vit_inspect_tensorboard_plugin\n"
     ]
    }
   ],
   "source": [
    "%cd ../vit_inspect_tensorboard_plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/stamatiad/Documents/GitHub\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KHEyXX3AJIX",
    "outputId": "83a1c83a-ac2e-47b8-ca39-ad9df6e9512e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/stamatiad/Documents/GitHub\n"
     ]
    }
   ],
   "source": [
    "# Get out from the vit_inspect folder, into the parent directory of your git repositories:\n",
    "if not on_colab:\n",
    "  %cd ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_imDskhXd1sc"
   },
   "source": [
    "Now clone and install the VI plugin, if not done already. It is important to use setuptools for the TB to register the plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMWGUIkt1GBz",
    "outputId": "5ef3ac30-50f3-4987-f56d-ab0a83e6510e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/stamatiad/Documents/GitHub/vit_inspect_tensorboard_plugin\n",
      "Already up to date.\n",
      "Processing /home/stamatiad/Documents/GitHub/vit_inspect_tensorboard_plugin\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hBuilding wheels for collected packages: vit-inspect\n",
      "  Building wheel for vit-inspect (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for vit-inspect: filename=vit_inspect-0.1.0-py3-none-any.whl size=170753 sha256=dbbf38c91b9a80fe93d078f29077cd32711e9eb36f00a473f71a2d036038517f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-v6agivny/wheels/77/f6/ab/c0b9ac7f7bdb3ce58163a6c2f2db5c191b9f02f491640cd758\n",
      "Successfully built vit-inspect\n",
      "Installing collected packages: vit-inspect\n",
      "  Attempting uninstall: vit-inspect\n",
      "    Found existing installation: vit-inspect 0.1.0\n",
      "    Uninstalling vit-inspect-0.1.0:\n",
      "      Successfully uninstalled vit-inspect-0.1.0\n",
      "Successfully installed vit-inspect-0.1.0\n",
      "/home/stamatiad/Documents/GitHub\n"
     ]
    }
   ],
   "source": [
    "if not Path(os.getcwd()).joinpath('vit_inspect_tensorboard_plugin').exists():\n",
    "  !git clone https://stamatiad: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@github.com/stamatiad/vit_inspect_tensorboard_plugin.git\n",
    "%cd vit_inspect_tensorboard_plugin\n",
    "# Make sure repo is up to date:\n",
    "!git pull\n",
    "# Install using setup tools:\n",
    "!pip install .\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pO2-LoPdLf2"
   },
   "source": [
    "Now clone the repo of the PyTorch example model and checkout our customized branch to see\n",
    "the changes required to run it along with the VI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfCQhdRJdLf5",
    "outputId": "55f162b9-203d-466d-f132-4729164eda45",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/stamatiad/Documents/GitHub/dino\n",
      "Already up to date.\n",
      "M\tvi_example.py\n",
      "D\tvi_logs/events.out.tfevents.1672074953.stamatiad-laptop.9563.0.v2\n",
      "D\tvi_logs/events.out.tfevents.1674386022.stamatiad-laptop.5141.0.v2\n",
      "Already on 'stamatiad'\n",
      "Your branch is up to date with 'origin/stamatiad'.\n"
     ]
    }
   ],
   "source": [
    "if not Path(os.getcwd()).joinpath('dino').exists():\n",
    "  !git clone https://github.com/stamatiad/dino.git\n",
    "%cd dino\n",
    "# Make sure repo is up to date:\n",
    "!git pull\n",
    "# Checkout our custom branch, that integrates VI:\n",
    "!git checkout stamatiad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QMTg2nadLgn"
   },
   "source": [
    "As you can see, we have created a wrapper function (save_attn_weights) that\n",
    "wraps the forward method of Attention. Now each time the forward method is\n",
    "called and we have VI recording enabled (with vi.enable_vi() context manager)\n",
    ", we will save the TB summaries in the directory ./vi_logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TbwYCpyrdLgt",
    "outputId": "62cbda36-2eac-4244-ae4e-338be6774344",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mdiff --git a/vision_transformer.py b/vision_transformer.py\u001B[m\r\n",
      "\u001B[1mindex f69a7ad..67bb43f 100644\u001B[m\r\n",
      "\u001B[1m--- a/vision_transformer.py\u001B[m\r\n",
      "\u001B[1m+++ b/vision_transformer.py\u001B[m\r\n",
      "\u001B[36m@@ -23,6 +23,72 @@\u001B[m \u001B[mimport torch.nn as nn\u001B[m\r\n",
      " \u001B[m\r\n",
      " from utils import trunc_normal_\u001B[m\r\n",
      " \u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m# VI imports:\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32mfrom functools import wraps\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32mimport numpy as np\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32mimport json\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32mimport tensorflow as tf\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32mfrom vit_inspect import vit_inspector as vi\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32mfrom vit_inspect.summary_v2 import vi_summary\u001B[m\r\n",
      "\u001B[32m+\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32mdef save_attn_weights():\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m    # Zero indexed layer counter:\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m    layer_counter = 0\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m    def decorator(func):\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m        @wraps(func)\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m        def wrapper(*args, **kw):\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m            # Get access to the variable:\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m            nonlocal layer_counter\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m            try:\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                # Evaluate the attention function first:\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                x, w = func(*args, **kw)\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                # If we are recording with VI:\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                if vi._summary_is_active:\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    # Number of tokens:\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    nt = vi.params[\"num_tokens\"]\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    # Patch size\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    ps = int(np.sqrt(nt))\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    # Attention heads:\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    ah = vi.params[\"num_heads\"]\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    # Remove class dim and partition into image patches.\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    tmp = np.asarray(\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                        w[:, :, :nt, :nt].reshape(-1, ah, nt, ps, ps)\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    )\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    # Preprocess to image with cmap, in order to save with\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    # images TF writer.\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    attn_maps = vi.maps_to_imgs(tmp)\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    batch_id = vi.params[\"batch_id\"]\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    num_layers = vi.params[\"num_layers\"]\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    #Get active layer to add to the summary tag:\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    active_layer = np.mod(layer_counter, num_layers)\u001B[m\r\n",
      "\u001B[32m+\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    # Convert to TF tensor, to save with summary writer:\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    flat_arr_w = tf.convert_to_tensor(\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                        np.reshape(attn_maps, [-1, *attn_maps.shape[-3:]])\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    )\u001B[m\r\n",
      "\u001B[32m+\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    with vi.writer.as_default():\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                        vi_summary(\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                            f\"b{batch_id}l{active_layer}\",\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                            flat_arr_w,\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                            step=vi.params[\"step\"],\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                            # Provide all the relevant model params to the TB\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                            # plugin:\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                            description=json.dumps(vi.params)\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                        )\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                        vi.writer.flush()\u001B[m\r\n",
      "\u001B[32m+\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    # Increment the layer counter, keeping track for the\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    # active layer.\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                    layer_counter += 1\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m            except Exception as e:\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                print(f\"Something in JAX broke: {e}\")\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m                raise e\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m            # Return the results of the attention function that we called\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m            # earlier:\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m            return x, w #=func(*args, **kw)\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m        return wrapper\u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m    return decorator\u001B[m\r\n",
      " \u001B[m\r\n",
      " def drop_path(x, drop_prob: float = 0., training: bool = False):\u001B[m\r\n",
      "     if drop_prob == 0. or not training:\u001B[m\r\n",
      "\u001B[36m@@ -77,6 +143,7 @@\u001B[m \u001B[mclass Attention(nn.Module):\u001B[m\r\n",
      "         self.proj = nn.Linear(dim, dim)\u001B[m\r\n",
      "         self.proj_drop = nn.Dropout(proj_drop)\u001B[m\r\n",
      " \u001B[m\r\n",
      "\u001B[32m+\u001B[m\u001B[32m    @save_attn_weights()\u001B[m\r\n",
      "     def forward(self, x):\u001B[m\r\n",
      "         B, N, C = x.shape\u001B[m\r\n",
      "         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\u001B[m\r\n"
     ]
    }
   ],
   "source": [
    "!git diff main..stamatiad -- vision_transformer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZyraqundLg_"
   },
   "source": [
    "Now lets run our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "mLneb6WcdLhA"
   },
   "outputs": [],
   "source": [
    "# Work on the original DINO with PyTorch:\n",
    "\n",
    "# VI imports:\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from vit_inspect import vit_inspector as vi\n",
    "from vit_inspect.summary_v2 import vi_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "gL12tmCZnJcs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as pth_transforms\n",
    "from vision_transformer import VisionTransformer\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c7C-oCYdLhE"
   },
   "source": [
    "Load the pre-trained model and transfer its parameters to a new instance of\n",
    "our modified model, that VI listens to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8KosQFNdLhG",
    "outputId": "e93f7079-7b0a-4f4b-fb69-aaf1d6124898",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/stamatiad/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained model:\n",
    "model_cached = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "# Create a version of the model that holds our attention VI modifications:\n",
    "# Match model params, before load:\n",
    "num_features = model_cached.embed_dim\n",
    "model = VisionTransformer(embed_dim=num_features)\n",
    "model.load_state_dict(model_cached.state_dict(), strict=False)\n",
    "\n",
    "# Enable evaluation mode:\n",
    "device = torch.device(\"cpu\")\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5WAtl-bdLhJ"
   },
   "source": [
    "Initialize the VI and inform it about our model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "6mxKiFZBdLhL"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (PosixPath('vi_logs/2023-02-14_10-13-47')) with an unsupported type (<class 'pathlib.PosixPath'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [40], line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Define summary location:\u001B[39;00m\n\u001B[1;32m     13\u001B[0m summaries_path \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvi_logs\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mjoinpath(datetime\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mH-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mM-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mS\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m---> 14\u001B[0m vi\u001B[38;5;241m.\u001B[39mwriter \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msummary\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_file_writer\u001B[49m\u001B[43m(\u001B[49m\u001B[43msummaries_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/komod/lib/python3.9/site-packages/tensorflow/python/ops/summary_ops_v2.py:531\u001B[0m, in \u001B[0;36mcreate_file_writer_v2\u001B[0;34m(logdir, max_queue, flush_millis, filename_suffix, name, experimental_trackable)\u001B[0m\n\u001B[1;32m    524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[1;32m    525\u001B[0m   _check_create_file_writer_args(\n\u001B[1;32m    526\u001B[0m       inside_function,\n\u001B[1;32m    527\u001B[0m       logdir\u001B[38;5;241m=\u001B[39mlogdir,\n\u001B[1;32m    528\u001B[0m       max_queue\u001B[38;5;241m=\u001B[39mmax_queue,\n\u001B[1;32m    529\u001B[0m       flush_millis\u001B[38;5;241m=\u001B[39mflush_millis,\n\u001B[1;32m    530\u001B[0m       filename_suffix\u001B[38;5;241m=\u001B[39mfilename_suffix)\n\u001B[0;32m--> 531\u001B[0m logdir \u001B[38;5;241m=\u001B[39m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_to_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogdir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstring\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    532\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m max_queue \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    533\u001B[0m   max_queue \u001B[38;5;241m=\u001B[39m constant_op\u001B[38;5;241m.\u001B[39mconstant(\u001B[38;5;241m10\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/komod/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:183\u001B[0m, in \u001B[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    181\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m Trace(trace_name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtrace_kwargs):\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 183\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/komod/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1636\u001B[0m, in \u001B[0;36mconvert_to_tensor\u001B[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001B[0m\n\u001B[1;32m   1627\u001B[0m       \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1628\u001B[0m           _add_error_prefix(\n\u001B[1;32m   1629\u001B[0m               \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConversion function \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconversion_func\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m for type \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1632\u001B[0m               \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mactual = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mret\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mbase_dtype\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1633\u001B[0m               name\u001B[38;5;241m=\u001B[39mname))\n\u001B[1;32m   1635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1636\u001B[0m   ret \u001B[38;5;241m=\u001B[39m \u001B[43mconversion_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mas_ref\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mas_ref\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1638\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m:\n\u001B[1;32m   1639\u001B[0m   \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/komod/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:343\u001B[0m, in \u001B[0;36m_constant_tensor_conversion_function\u001B[0;34m(v, dtype, name, as_ref)\u001B[0m\n\u001B[1;32m    340\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_constant_tensor_conversion_function\u001B[39m(v, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    341\u001B[0m                                          as_ref\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    342\u001B[0m   _ \u001B[38;5;241m=\u001B[39m as_ref\n\u001B[0;32m--> 343\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconstant\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/komod/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:267\u001B[0m, in \u001B[0;36mconstant\u001B[0;34m(value, dtype, shape, name)\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconstant\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[])\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconstant\u001B[39m(value, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, shape\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConst\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    172\u001B[0m   \u001B[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001B[39;00m\n\u001B[1;32m    173\u001B[0m \n\u001B[1;32m    174\u001B[0m \u001B[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_constant_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverify_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mallow_broadcast\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/komod/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:279\u001B[0m, in \u001B[0;36m_constant_impl\u001B[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001B[0m\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m trace\u001B[38;5;241m.\u001B[39mTrace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf.constant\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    278\u001B[0m       \u001B[38;5;28;01mreturn\u001B[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001B[0;32m--> 279\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_constant_eager_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverify_shape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    281\u001B[0m g \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mget_default_graph()\n\u001B[1;32m    282\u001B[0m tensor_value \u001B[38;5;241m=\u001B[39m attr_value_pb2\u001B[38;5;241m.\u001B[39mAttrValue()\n",
      "File \u001B[0;32m~/miniconda3/envs/komod/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:304\u001B[0m, in \u001B[0;36m_constant_eager_impl\u001B[0;34m(ctx, value, dtype, shape, verify_shape)\u001B[0m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_constant_eager_impl\u001B[39m(ctx, value, dtype, shape, verify_shape):\n\u001B[1;32m    303\u001B[0m   \u001B[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 304\u001B[0m   t \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_to_eager_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    305\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m shape \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    306\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\n",
      "File \u001B[0;32m~/miniconda3/envs/komod/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:102\u001B[0m, in \u001B[0;36mconvert_to_eager_tensor\u001B[0;34m(value, ctx, dtype)\u001B[0m\n\u001B[1;32m    100\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m dtypes\u001B[38;5;241m.\u001B[39mas_dtype(dtype)\u001B[38;5;241m.\u001B[39mas_datatype_enum\n\u001B[1;32m    101\u001B[0m ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m--> 102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEagerTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mValueError\u001B[0m: Attempt to convert a value (PosixPath('vi_logs/2023-02-14_10-13-47')) with an unsupported type (<class 'pathlib.PosixPath'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "# Get some model params, required for VI:\n",
    "vi.params[\"num_layers\"] = len(model.blocks)\n",
    "vi.params[\"num_heads\"] = model.blocks[0].attn.num_heads\n",
    "# The number of tokens when the attention dot product happens.\n",
    "# Here tokens are the patches. Any other feature (e.g. class) is removed.\n",
    "patch_size = model.patch_embed.patch_size\n",
    "crop_size = 480\n",
    "img_size_in_patches = crop_size // patch_size\n",
    "vi.params[\"len_in_patches\"] = img_size_in_patches\n",
    "# Total patches in the image:\n",
    "vi.params[\"num_tokens\"] = img_size_in_patches ** 2\n",
    "# Define summary location:\n",
    "summaries_path = Path(\"vi_logs\").joinpath(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "vi.writer = tf.summary.create_file_writer(str(summaries_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPAprINAdLhN"
   },
   "source": [
    "Load a sample image to calculate attention maps uppon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "j3Hsb-O_dLhO"
   },
   "outputs": [],
   "source": [
    "# Load sample images:\n",
    "response = requests.get(\"https://dl.fbaipublicfiles.com/dino/img.png\")\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img = img.convert('RGB')\n",
    "\n",
    "# Perform the original transformations that the authors did.\n",
    "transform = pth_transforms.Compose([\n",
    "    pth_transforms.Resize(img.size),\n",
    "    pth_transforms.ToTensor(),\n",
    "    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "img = transform(img)\n",
    "# make the image divisible by the patch size\n",
    "w, h = img.shape[1] - img.shape[1] % patch_size, \\\n",
    "       img.shape[2] - img.shape[2] % patch_size\n",
    "img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "w_featmap = img.shape[-2] // patch_size\n",
    "h_featmap = img.shape[-1] // patch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHWDEybLdLhP"
   },
   "source": [
    "Save a copy of the input image for the VI to display it as preview, making it\n",
    " easier to visualize the attention maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "lHdBYcvOdLhz"
   },
   "outputs": [],
   "source": [
    "# Save the input image into the summary:\n",
    "flat_arr_rgb = tf.convert_to_tensor(\n",
    "    # Make sure image's channels is the last dim:\n",
    "    np.moveaxis(np.asarray(img), 1, -1)\n",
    ")\n",
    "with vi.writer.as_default():\n",
    "    step = 0\n",
    "    batch_id = 0\n",
    "    vi.params[\"step\"] = 0\n",
    "    vi.params[\"batch_id\"] = batch_id\n",
    "    vi_summary(\n",
    "        f\"b{batch_id}\",\n",
    "        flat_arr_rgb,\n",
    "        step=step,\n",
    "        description=json.dumps(vi.params)\n",
    "    )\n",
    "    vi.writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rkAwA3LdLh1"
   },
   "source": [
    "Finally, perform inference with VI enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JsWc0V9adLh2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 09:44:56.688082: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 233280000 exceeds 10% of free system memory.\n",
      "2023-02-14 09:44:56.899545: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 233280000 exceeds 10% of free system memory.\n",
      "2023-02-14 09:44:56.986819: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 233280000 exceeds 10% of free system memory.\n",
      "2023-02-14 09:44:57.216399: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 233280000 exceeds 10% of free system memory.\n",
      "2023-02-14 09:44:57.449341: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 29160000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# Use the VI context manager to get attention maps of each layer and head:\n",
    "with vi.enable_vi():\n",
    "    attentions = model.get_last_selfattention(img.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "id": "03XpOC6ZdLh3",
    "outputId": "b3ebca87-3505-4b3f-c41e-88b6329ca5c1",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Ykc8BiBcdLh4",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 4889), started 0:01:30 ago. (Use '!kill 4889' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4a2c0f912708da39\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4a2c0f912708da39\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir vi_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
